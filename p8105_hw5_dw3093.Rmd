---
title: "p8105_hw5_dw3093"
author: "Katherine Wang"
output: github_document
---

```{r include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(rvest)
library(broom)
set.seed(1)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
  )
theme_set(theme_minimal()+ theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

#Question 1
```{r}
shared_bday <- function(n) {
  bday <- sample(1:365, size = n, replace = TRUE) 
  return(any(duplicated(bday)))
}
group_sizes <- 2:50
n_simulation <- 10000

prob <- numeric(length(group_sizes))
for (i in seq_along(group_sizes)) {
  group_size <- group_sizes[i]

  results <- replicate(n_simulation, shared_bday(group_size))
  prob[i] <- mean(results)
}
results <- data.frame(
  group_size = group_sizes,
  probability = prob
)

ggplot(results, aes(x = group_size, y = probability)) +
  geom_line() +
  geom_point(alpha=.6) +
  labs(
    title = "Probability of Shared Birthday as a Function of Group Size",
    x = "Group Size (n)",
    y = "Probability of Shared Birthday"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```
We can conclude that the probability of a shared birthday is low with small group sizes. However, as the group size increases, the probability rises sharply, reaching over 50% by around 23 people and approaching near certainty by 50 people. 


# Question 2
# Set up parameters as question required
```{r}
n <- 30
sigma <- 5
alpha <- 0.05
mu_values <- 0:6
n_simulations <- 5000
```

# Function to simulate data
```{r}
t_test <- function(mu) {
  sample_data <- rnorm(n = n, mean = mu, sd = sigma)
  t_stats <- t.test(sample_data, mu = 0) |>
    broom::tidy() |>
    select(estimate, p.value)
  return(t_stats)
}
```

# Generate simulation results for each mu value
```{r}
simulation_results <- tibble(
  mu = rep(mu_values, each = n_simulations)
) |>
  mutate(
    t_test_output = map(mu, t_test)
  ) |>
  unnest(cols = t_test_output)
```

# Calculate power and average estimates
```{r}
summary_results <- simulation_results |>
  group_by(mu) |>
  summarize(
    power = mean(p.value < alpha),
    avg_est_all = mean(estimate),
    avg_est_rej = mean(estimate[p.value < alpha], na.rm = TRUE),
    .groups = 'drop'
  )
```

# Plot 1: Power of the t-test vs. True Value of μ
```{r}
ggplot(summary_results, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Power of the t-test vs. True Value of μ",
    x = "True Value of μ",
    y = "Power (Proportion of Null Rejections)"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
)
```
This plot shows how the true value of μ influences the power of the t-test. As the true mean μ increases, the power of the test rises correspondingly. This trend reflects a greater likelihood of rejecting the null hypothesis as the effect size grows, suggesting that the test becomes more effective at detecting a true difference from 0 when μ is larger. Consequently, higher values of μ yield a higher probability of rejecting the null hypothesis rises

# Plot 2: Average Estimate vs. True Mean (μ)
```{r}
ggplot(summary_results, aes(x = mu)) +
  geom_line(aes(y = avg_est_all, color = "All Samples")) +
  geom_point(aes(y = avg_est_all, color = "All Samples")) +
  geom_line(aes(y = avg_est_rej, color = "Null Rejected Samples")) +
  geom_point(aes(y = avg_est_rej, color = "Null Rejected Samples")) +
  labs(
    title = "Average Estimate of Sample Mean (μ̂) vs. True Mean (μ)",
    x = "True Mean (μ)",
    y = "Average Estimate of Sample Mean (μ̂)",
    color = "Sample Type"
  ) +
  scale_color_manual(
    values = c("All Samples" = "orange", "Null Rejected Samples" = "pink"),
    labels = c("All Samples", "Null Rejected Samples")
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold")
)
```
This plot shows the average estimate of μ^ for all samples (orange) and for only those samples where the null was rejected (pink), across different values of the true mean μ.

No, the sample average of μ^ across tests where the null hypothesis is rejected is not approximately equal to the true value of μ, especially for smaller values of μ. 

This discrepancy arises due to selection bias. When we focus only on tests that reject the null hypothesis, we tend to include samples with larger-than-average estimates of μ^, as these are more likely to yield significant p-values. Consequently, the observed average in this subset is inflated. However, as the true value of μ becomes larger, this bias diminishes because the true effect is strong enough that a broader range of sample estimates lead to significance, resulting in an average estimate closer to the true value of μ.

# Question 3
# Load dataset from github
```{r}
homicide = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv")
summary(homicide)
```
This dataset contains `r nrow(homicide)` records on homicide incidents, each with `r ncol(homicide)` columns. 
Each entry includes a unique identifier (uid), report date (reported_date), and victim details such as last name (victim_last), first name (victim_first), race (victim_race), age (victim_age), and sex (victim_sex). Location is specified by city, state, and geographic coordinates (lat and lon). The (disposition) shows the case outcome. 

I found there is a repeated entry of Tulsa, AL, and Tulsa, OK, it seems a data entry error, by Google the city of Tulsa, I noticed that Tulsa, AL is wrong. So, will conduct further data cleaning. 

# Create city_state variable and summarize
```{r}
homicide_sum <- homicide %>%
  mutate(city_state = paste(city, state, sep = ", ")) %>%
  group_by(city_state) %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")),
    .groups = 'drop'
  ) %>% 
  filter(city_state!="Tulsa, AL")

knitr::kable(homicide_sum)
```
Here, the city_state variable is created to combine city and state names, and then a summary of total and unsolved homicides is generated for each city. The city "Tulsa, AL" is filtered out bef$ore displaying the table.

```{r}
baltimore_data <- homicide_sum %>%
  filter(city_state == "Baltimore, MD")

baltimore_test <- prop.test(
  x = baltimore_data$unsolved_homicides,
  n = baltimore_data$total_homicides
) %>%
  broom::tidy()

baltimore_result <- baltimore_test %>% 
  select(estimate, conf.low, conf.high) %>%
  mutate(city_state = "Baltimore, MD")

knitr::kable(baltimore_result, 
             col.names = c("Estimate", "Lower CI", "Upper CI", "City, State"))
```
I filtered the dataset for "Baltimore, MD" and use prop.test to estimate the proportion of homicides that remain unsolved. The output from prop.test is processed with broom::tidy() to extract the estimated proportion and confidence interval. This table shows that approximately 64.6% of homicides in Baltimore, MD, remain unsolved, with a 95% confidence interval of 62.8% to 66.3%.

```{r}
unsolved_prop <- function(unsolved, total){
  prop_test_result <- prop.test(unsolved, total)
  broom::tidy(prop_test_result) %>% 
    select(estimate, conf.low, conf.high)
}
homicide_sum <- homicide_sum %>% 
  mutate(
    prop_results = purrr::map2(unsolved_homicides, 
                               total_homicides, 
                               ~unsolved_prop(.x, .y)
                               )
  ) %>%
  unnest(prop_results)

knitr::kable(homicide_sum)
```

```{r}
homicide_sum |>
mutate(
    city_state = reorder(city_state, estimate)
  )|>
ggplot(aes(x = city_state, y = estimate)) +
  geom_point(color = "pink") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City, State",
    y = "Proportion of Unsolved Homicides"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text = element_text(size = 8),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )
```
This plot displays the estimated proportions of unsolved homicides for each city, with error bars representing the 95% CI. Cities are organized in ascending order of unsolved homicide proportions, ranging from around 30% in Richmond, VA, to around 75% in New Orleans, LA. 